I"ì´<p>This article is about my <a href="https://arxiv.org/abs/2205.07076">research paper</a> (accepted at IJCNN 2022) where two methods of spiking-MaxPooling in Convolutional Spiking Neural Networks (SNNs) are proposed; both these methods are entirely deployable on the Loihi neuromorphic hardware.</p>

<h4 id="learning-objectives">Learning objectives:</h4>
<ul>
  <li>How to do spiking-MaxPooling in Convolutional SNNs?</li>
</ul>

<p>This article only serves as a Proof-of-Concept (PoC) demonstration of the two proposed methods with <a href="https://github.com/R-Gaurav/SpikingMaxPooling/blob/main/misc/Paper_324_Proof_Of_Concept_Demo.ipynb">minimal code</a> to deploy them on Loihi. For more details on the entire suite of experiments, results, and analysis please go through the paper linked above. For complete code of the experiments please visit my Github <a href="https://github.com/R-Gaurav/SpikingMaxPooling">repo</a>.</p>

<h2 id="whats-the-problem-statement">What‚Äôs the Problem Statement?</h2>

<blockquote>
  <p><code class="language-plaintext highlighter-rouge">How to build Convolutional SNNs with MaxPooling layers such that they are entirely deployable on a neuromorphic hardware?</code></p>
</blockquote>

<p>In the Convolutional SNNs (henceforth just SNNs here), MaxPooling isn‚Äôt as trivial as the regular \(max(.)\) operation. What will you take \(max(.)\) of? binary spikes? Will it be optimal? A few methods of MaxPooling in SNNs exist (more details in the paper linked above), but <strong>none</strong> of them have been evaluated on Loihi - in the context of MaxPooling in SNNs. Therefore, we designed two neurmorphic-hardware friendly methods of spiking-MaxPooling in SNNs, and evaluated their efficacy with a number of Image Classification experiments on MNIST, FMNIST, and CIFAR10 datasets.</p>

<p>Note that in the absence of such neuromorphic-hardware friendly spiking-MaxPooling methods, a lot of <code class="language-plaintext highlighter-rouge">on-chip</code> - <code class="language-plaintext highlighter-rouge">off-chip</code> inter-communication will happen upon the deployment of SNNs with MaxPooling layers on a neuromorphic hardware. This is due to the fact that the <code class="language-plaintext highlighter-rouge">Conv</code> layers with neurons get deployed <code class="language-plaintext highlighter-rouge">on-chip</code> and <code class="language-plaintext highlighter-rouge">MaxPooling</code> layers with no neurons get deployed <code class="language-plaintext highlighter-rouge">off-chip</code>. Such unsought inter-communication not only defeats the energy-efficiency motive of deploying SNNs on a neuromorphic hardware, but also results in spike-communication latency.</p>

<h1 id="methods-of-spiking-maxpooling">Methods of spiking-MaxPooling</h1>

<p>I now present the theory and PoC demonstration of our proposed methods of spiking-MaxPooling, namely: <strong>MJOP</strong> and <strong>AVAM</strong>. Both of these methods rely on the representation of the artificial neuron <code class="language-plaintext highlighter-rouge">activations</code> as <code class="language-plaintext highlighter-rouge">currents</code> in the SNNs, obtained after filtering the spikes from the spiking neurons. Let‚Äôs consider the case of \(2\times2\) MaxPooling, where for one such pooling window, we need to find the \(max(U_1, U_2, U_3, U_4)\) where \(U_i\)s are the input current values. I used <a href="https://www.nengo.ai/nengo-loihi/">NengoLoihi</a> backend to deploy both these methods - <strong>MJOP</strong> and <strong>AVAM</strong> on the Loihi neuromorphic hardware. For the benchmark purpose, I obtained the <code class="language-plaintext highlighter-rouge">True Max U</code> = \(max(U_1, U_2, U_3, U_4)\) from a different network run on CPU, and compared the outputs of the <strong>MJOP</strong> and <strong>AVAM</strong> methods with <code class="language-plaintext highlighter-rouge">True Max U</code>. The evaluation criteria is to visually check if the <strong>MJOP</strong> and <strong>AVAM</strong> outputs closely match the \(max(.)\) output!</p>

<h2 id="max-join-op-mjop">MAX join-Op (MJOP)</h2>

<p>The <strong>MJOP</strong> method of spiking-MaxPooling is a Loihi <strong>dependent</strong> method, as it uses the NxCore APIs and the Multi-Compartment (MC) neuron properties of Loihi. Note the subtle difference between ‚Äúcompartments‚Äù and ‚Äúneurons‚Äù; a Loihi neuron can have one or more spiking units (called compartments) in it. MJOP method is loosely based on the simple observation that:</p>

\[U_{max} = max(U_1, max(U_2, max(U_3, U_4)))\]

<h3 id="description">Description</h3>
<p>In a MC neuron, the Single-Compartment (SC) units are connected in a binary tree fashion, and can communicate the received \(U_i\) to their parent compartment. Note that each compartment in a MC neuron can be stimulated by an external input \(U_i\). Therefore, considering the case of a two compartment neuron i.e. the parent compartment has only one kid, where both the compartments receive currents \(U_1\) and \(U_2\) respectively, the parent compartment has to act on the current \(U_2\) from its kid. It does so by executing one of the many <code class="language-plaintext highlighter-rouge">join</code> operations provided by the low-level Loihi APIs. These <code class="language-plaintext highlighter-rouge">join</code> Ops can be <code class="language-plaintext highlighter-rouge">MIN</code>, <code class="language-plaintext highlighter-rouge">ADD</code>, <code class="language-plaintext highlighter-rouge">MAX</code>, etc.. And as you might have guessed by now, we used the <code class="language-plaintext highlighter-rouge">MAX</code> <code class="language-plaintext highlighter-rouge">join</code>-Op and the MC neuron creation functionality in Loihi to realize spiking-MaxPooling in SNNs - as shown in the figure below (for a \(2\times2\) MaxPooling window).</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="MJOP-Net.png" width="500" height="400" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><b> Fig taken from our paper - <i>MJOP Net</i> for \(2\times2\) MaxPooling </b></td>
    </tr>
  </tbody>
</table>

<p>The topmost root neuron receives a running \(max(.)\) of all the input currents, and then it spikes at a rate corresponding to the maximum computed current \(U_{max}\). Since it outputs spikes (and not the maximum current) at a rate directly proportional to the maximum input current \(U_{max}\), we need to <em><strong>scale</strong></em> the filtered output spikes to match it to the true maximum current. Had the root neuron been able to communicate current to the next connected neuron on Loihi, outputting a simple \(max(.)\) of currents would have been possible; but this is not the case here. Note that, the required number of compartments in the MC neuron is same as the number of elements in the MaxPooling window. Also note that the value of <em><strong>scale</strong></em> depends on a number of factors, e.g. the root neuron‚Äôs configuration and the maximum input current to the root neuron. More details on how to choose the <em><strong>scale</strong></em> value are in our paper.</p>

<h3 id="2times2-spiking-maxpooling-poc-code">\(2\times2\) spiking-MaxPooling PoC Code</h3>
<p>Since NengoLoihi uses the NxCore APIs (and not the NxNet APIs), I had to use the low level NxCore APIs to configure the <code class="language-plaintext highlighter-rouge">Ensemble</code> of neurons to a MC neuron on the Loihi hardware. In short, for a \(2\times2\) MaxPooling, you need to create an <code class="language-plaintext highlighter-rouge">Ensemble</code> \(4\) neurons, then access the NengoLoihi object mapping the <code class="language-plaintext highlighter-rouge">Ensemble</code> to the Loihi board, and then configure the individual neurons (now considered as ‚Äúcompartments‚Äù on the Loihi board) to create a MC neuron with <code class="language-plaintext highlighter-rouge">MAX</code> join-Op between the compartments. I named such a network of compartments as the <strong>MJOP</strong> Net - in the figure above.</p>

<p>Following is the minimal PoC code for creating the <strong>MJOP</strong> Net:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">configure_ensemble_for_2x2_max_join_op</span><span class="p">(</span><span class="n">loihi_sim</span><span class="p">,</span> <span class="n">ens</span><span class="p">):</span>
  <span class="s">"""
  Configures a Nengo Ensemble to create multiple Multi-Compartment Neurons with
  4 compartments and MAX join-Op between those compartments.

  Args:
    loihi_sim &lt;nengo_loihi.simulator.Simulator&gt;: NengoLoihi simulator object.
    ens &lt;nengo.ensemble.Ensemble&gt;: The Ensemble whose neurons are supposed to be
                                   configured.
  """</span>

  <span class="n">nxsdk_board</span> <span class="o">=</span> <span class="n">loihi_sim</span><span class="p">.</span><span class="n">sims</span><span class="p">[</span><span class="s">"loihi"</span><span class="p">].</span><span class="n">nxsdk_board</span>
  <span class="n">board</span> <span class="o">=</span> <span class="n">loihi_sim</span><span class="p">.</span><span class="n">sims</span><span class="p">[</span><span class="s">"loihi"</span><span class="p">].</span><span class="n">board</span>

  <span class="c1"># Get the blocks (which can be many depending on how large the Ensemble `ens`
</span>  <span class="c1"># is and in how many blocks is it broken).
</span>  <span class="n">blocks</span> <span class="o">=</span> <span class="n">loihi_sim</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">objs</span><span class="p">[</span><span class="n">ens</span><span class="p">]</span>
  <span class="c1">#print("Number of (in and out) Blocks for Ensemble %s are: %s and %s."
</span>  <span class="c1">#          % (ens, len(blocks["in"]), len(blocks["out"])))
</span>  <span class="k">for</span> <span class="n">block</span> <span class="ow">in</span> <span class="n">blocks</span><span class="p">[</span><span class="s">"in"</span><span class="p">]:</span>
    <span class="n">in_chip_idx</span><span class="p">,</span> <span class="n">in_core_idx</span><span class="p">,</span> <span class="n">in_block_idx</span><span class="p">,</span> <span class="n">in_compartment_idxs</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">board</span><span class="p">.</span><span class="n">find_block</span><span class="p">(</span><span class="n">block</span><span class="p">))</span>
    <span class="n">nxsdk_core</span> <span class="o">=</span> <span class="n">nxsdk_board</span><span class="p">.</span><span class="n">n2Chips</span><span class="p">[</span><span class="n">in_chip_idx</span><span class="p">].</span><span class="n">n2CoresAsList</span><span class="p">[</span><span class="n">in_core_idx</span><span class="p">]</span>

    <span class="c1"># Set the cxProfileCfg[0] as the leaf node's profile with `stackOut=3` =&gt;
</span>    <span class="c1"># it pushes the current U to the top of the stack.
</span>    <span class="n">nxsdk_core</span><span class="p">.</span><span class="n">cxProfileCfg</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">configure</span><span class="p">(</span><span class="n">stackOut</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">bapAction</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">refractDelay</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="c1"># Set the cxProfileCfg[1] as the intermediate node's profile with `stackIn=2`
</span>    <span class="c1"># =&gt; it pops the element from the stack, `joinOp=2` =&gt; it does the MAX joinOp
</span>    <span class="c1"># with the popped element from stack and its current U, `stackOut=3` =&gt; it
</span>    <span class="c1"># pushes the MAXed current U on the top of the stack,
</span>    <span class="c1"># `decayU=nxsdk_core.cxProfileCfg[0].decayU` =&gt; the decay constant for current
</span>    <span class="c1"># U is same as that of the cxProfileCfg[0]. If `decayU` is 0, the current due
</span>    <span class="c1"># incoming spike never decays resulting in constant spiking of the neuron
</span>    <span class="c1"># and if it is default value, then the current decays instantly.
</span>    <span class="n">nxsdk_core</span><span class="p">.</span><span class="n">cxProfileCfg</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="n">configure</span><span class="p">(</span>
        <span class="n">stackIn</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">joinOp</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stackOut</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">decayU</span><span class="o">=</span><span class="n">nxsdk_core</span><span class="p">.</span><span class="n">cxProfileCfg</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">decayU</span><span class="p">)</span>
    <span class="c1"># Set the root node which will output the spikes corresonding to the MAXed U.
</span>    <span class="n">nxsdk_core</span><span class="p">.</span><span class="n">cxProfileCfg</span><span class="p">[</span><span class="mi">2</span><span class="p">].</span><span class="n">configure</span><span class="p">(</span>
        <span class="n">stackIn</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">joinOp</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">decayU</span><span class="o">=</span><span class="n">nxsdk_core</span><span class="p">.</span><span class="n">cxProfileCfg</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">decayU</span><span class="p">)</span>

    <span class="c1"># Set the compartments now.
</span>    <span class="c1"># Since the incoming connection from the previous Conv layer already as the
</span>    <span class="c1"># inputs in order of grouped slices, they are simply connected to the neuron
</span>    <span class="c1"># in this Ensembel `ens` from 0 index onwards.
</span>    <span class="c1"># `in_compartment_idxs` has the mapping of all compartment neurons in a
</span>    <span class="c1"># specific core, starting from index 0.
</span>
    <span class="c1"># Maximum number of compartment idxs = 1024.
</span>    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">in_compartment_idxs</span><span class="p">),</span> <span class="mi">4</span><span class="p">):</span>
      <span class="n">c_idx</span> <span class="o">=</span> <span class="n">in_compartment_idxs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
      <span class="c1"># Set a leaf node/compartment.
</span>      <span class="n">nxsdk_core</span><span class="p">.</span><span class="n">cxCfg</span><span class="p">[</span><span class="n">c_idx</span><span class="p">].</span><span class="n">configure</span><span class="p">(</span><span class="n">cxProfile</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">vthProfile</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
      <span class="c1"># Set two intermediate nodes/compartments.
</span>      <span class="n">nxsdk_core</span><span class="p">.</span><span class="n">cxCfg</span><span class="p">[</span><span class="n">c_idx</span><span class="o">+</span><span class="mi">1</span><span class="p">].</span><span class="n">configure</span><span class="p">(</span><span class="n">cxProfile</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">vthProfile</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
      <span class="n">nxsdk_core</span><span class="p">.</span><span class="n">cxCfg</span><span class="p">[</span><span class="n">c_idx</span><span class="o">+</span><span class="mi">2</span><span class="p">].</span><span class="n">configure</span><span class="p">(</span><span class="n">cxProfile</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">vthProfile</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
      <span class="c1"># Set a root node/compartment to output spikes corresponding to MAX input.
</span>      <span class="n">nxsdk_core</span><span class="p">.</span><span class="n">cxCfg</span><span class="p">[</span><span class="n">c_idx</span><span class="o">+</span><span class="mi">3</span><span class="p">].</span><span class="n">configure</span><span class="p">(</span><span class="n">cxProfile</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">vthProfile</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div></div>

<p>Following is the plot showing the scaled output from the <strong>MJOP</strong> Net compared to the <code class="language-plaintext highlighter-rouge">True Max U</code>.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="max_jop_poc.png" width="500" height="350" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><b> MJOP Net PoC Plot </b></td>
    </tr>
  </tbody>
</table>

<h2 id="absolute-value-based-associative-max-avam">Absolute Value based Associative Max (AVAM)</h2>

<p>The <strong>AVAM</strong> method of spiking-MaxPooling is a neuromorphic-hardware <strong>independent</strong> method, such that it can be deployed on any hardware (CPU/GPU/Loihi etc.) which supports spiking neurons and the filtering of spikes. It is based on the following two equations:</p>

\[max(a, b) = \frac{a+b}{2} + \frac{|a-b|}{2}\]

\[max(a, b, c, d) = max(max(a, b), max(c, d))\]

<p>where \(a\), \(b\), \(c\), and \(d\) can be the currents \(U_1\), \(U_2\), \(U_3\), and \(U_4\) respectively, and |.| is the absolute value function. Note that the second equation can be extended to any number of arguments.</p>

<h3 id="description-1">Description</h3>

<p>The average term \(\frac{a+b}{2}\) can be easily implemented on Loihi, as it is a simple linear operation. Recollect that AveragePooling can be easily implemented through the weighted connections on Loihi. The challenge is to implement the non-linear absolute value function i.e. |.| on Loihi with the linear weighted connections and non-linear spiking-neurons. How to do that?</p>

<h3 id="---approximation">| . | approximation</h3>

<p>One fine day, while staring at the plot of |.| function (shown below), it struck to us that we can configure two spiking-neurons such that their <code class="language-plaintext highlighter-rouge">Tuning Curves</code> would look similar to the graph of |x|.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="mod_x.png" width="600" height="350" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><b> |x| Plot </b></td>
    </tr>
  </tbody>
</table>

<p>What are the <code class="language-plaintext highlighter-rouge">Tuning Curves</code>?</p>

<blockquote>
  <p><code class="language-plaintext highlighter-rouge">Tuning Curves visually describe the activation profile of spiking neurons for an input stimuli.</code></p>
</blockquote>

<p>Therefore, we configured an <code class="language-plaintext highlighter-rouge">Ensemble</code> of two Integrate &amp; Fire (IF) spiking neurons - one with a positive encoder value of \(1\), another with a negative encoder value of \(-1\) (figure below), i.e. one neuron fires for a positive input (while the other does not), and the another neuron fires for a negative input (while the other does not).</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="tuning_curves.png" width="600" height="350" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><b> Tuning Curves Plot. x-axis -&gt; Input, y-axis -&gt; Output firing rate.</b> For a negative input, the neuron with orange tuning curve spikes, neuron with blue tuning curve does not. Vice versa, for a positive input, neuron with blue tuning curve spikes, neuron with orange tuning curve does not.</td>
    </tr>
  </tbody>
</table>

<p>Note that, no matter the sign of the input, such a system of two neurons outputs a positive firing rate upon stimulated with a signed input. However, we need to normalize the output firing rate to obtain the absolute value of the input \(x\).</p>

<p>There‚Äôs a caveat though, for a near accurate approximation of |x|, the representational <code class="language-plaintext highlighter-rouge">radius</code> of the <code class="language-plaintext highlighter-rouge">Ensemble</code> neurons should be equal to the <code class="language-plaintext highlighter-rouge">magnitude</code> of the $x$, i.e. <code class="language-plaintext highlighter-rouge">radius</code> = |x|. <em>Whaaat??</em> How do we then set the <code class="language-plaintext highlighter-rouge">radius</code> parameter of the <code class="language-plaintext highlighter-rouge">Ensemble</code> of spiking neurons when we do not know what \(x\) will be? It turns out that for binary spiking neurons, there are some heuristics we can use to effectively set the <code class="language-plaintext highlighter-rouge">radius</code> value! More details about these heuristics can be found in our paper.</p>

<h3 id="maxa-b-approximation">\(max(a, b)\) Approximation</h3>

<p>Combining the network representation for the linear average operation and the non-linear |.| operation, we obtain the following network - in the figure below, which quite well approximates the \(max(a, b)\) function.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="max_ab.png" width="370" height="350" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><b> <i>\(max(a, b)\) Network </i>. \(r\) is the radius value, \(\phi\) is the maximum firing rate. Purple A and B are the input nodes, O is the output node. Yellow circles with numbers \(1\) and \(2\) are the IF spiking neurons.</b> The output from the input nodes (A and B) are multiplied by \(\frac{1}{2}\) each and summed up at the output node (O) to get the average term \(\frac{a+b}{2}\). The output from the input nodes (A and B) also get multiplied by \(\frac{1}{2}\) and \(\frac{-1}{2}\) respectively to get the sum \(\frac{a-b}{2}\) input to the <code class="language-plaintext highlighter-rouge">Ensemble</code> of neurons. The output from the neurons is normalized with \(\phi\) and then multiplied by \(r\) to get the approximated \(\frac{|a-b|}{2}\), which is next added to the sum \(\frac{a+b}{2}\) at the output node (O) to finally output the approximated \(max(a, b)\). Note that only one of the two connections from the neurons to the output node (O) is active at a time, i.e. either the neuron \(1\) fires or the neuron \(2\) fires, not both.</td>
    </tr>
  </tbody>
</table>

<h3 id="2times2-spiking-maxpooling-poc-code-1">\(2\times2\) spiking-MaxPooling PoC Code</h3>

<p>The above network for the \(max(a, b)\) can be hierarchically stacked to compute the \(max(a, b, c, d)\) as follows, in the figure below; I call such an hierarchical network as the <strong>AVAM</strong> Net.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="avam.png" width="750" height="400" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><b> Figure taken from our paper - <i>AVAM Net</i> created with stacked \(max(a, b)\) Network</b></td>
    </tr>
  </tbody>
</table>

<p>Following is the minimal PoC code for creating the <strong>AVAM</strong> Net:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">get_loihi_adapted_avam_net_for_2x2_max_pooling</span><span class="p">(</span>
    <span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_rate</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">radius</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">do_max</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">synapse</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
  <span class="s">"""
  Returns a Loihi adapted network for absolute value based associative max pooling.

  Args:
    seed &lt;int&gt;: Any arbitrary seed value.
    max_rate &lt;int&gt;: Max Firing rate of the neurons.
    radius &lt;int&gt;: Value at which Maximum Firing rate occurs (
                  i.e. the representational radius)
    do_max &lt;bool&gt;: Do MaxPooling if True else do AvgPooling.
    synapse &lt;float&gt;: Synapic time constant.
  """</span>
  <span class="k">with</span> <span class="n">nengo</span><span class="p">.</span><span class="n">Network</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span> <span class="k">as</span> <span class="n">net</span><span class="p">:</span>
    <span class="n">net</span><span class="p">.</span><span class="n">inputs</span> <span class="o">=</span> <span class="n">nengo</span><span class="p">.</span><span class="n">Node</span><span class="p">(</span><span class="n">size_in</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span> <span class="c1"># 4 dimensional input for 2x2 MaxPooling.
</span>
    <span class="k">def</span> <span class="nf">_get_ensemble</span><span class="p">():</span>
      <span class="n">ens</span> <span class="o">=</span>  <span class="n">nengo</span><span class="p">.</span><span class="n">Ensemble</span><span class="p">(</span>
          <span class="n">n_neurons</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">dimensions</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">encoders</span><span class="o">=</span><span class="p">[[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]],</span> <span class="n">intercepts</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
          <span class="n">max_rates</span><span class="o">=</span><span class="p">[</span><span class="n">max_rate</span><span class="p">,</span> <span class="n">max_rate</span><span class="p">],</span> <span class="n">radius</span><span class="o">=</span><span class="n">radius</span><span class="p">,</span>
          <span class="n">neuron_type</span><span class="o">=</span><span class="n">nengo_loihi</span><span class="p">.</span><span class="n">neurons</span><span class="p">.</span><span class="n">LoihiSpikingRectifiedLinear</span><span class="p">())</span>
      <span class="k">return</span> <span class="n">ens</span>

    <span class="n">ens_12</span> <span class="o">=</span> <span class="n">_get_ensemble</span><span class="p">()</span> <span class="c1"># Ensemble for max(a, b).
</span>    <span class="n">ens_34</span> <span class="o">=</span> <span class="n">_get_ensemble</span><span class="p">()</span> <span class="c1"># Ensemble for max(c, d).
</span>    <span class="n">ens_1234</span> <span class="o">=</span> <span class="n">_get_ensemble</span><span class="p">()</span> <span class="c1"># Ensemble for max(max(a, b), max(c, d)).
</span>
    <span class="c1"># Intermediate passthrough nodes for summing and outputting the result.
</span>    <span class="n">node_12</span> <span class="o">=</span> <span class="n">nengo</span><span class="p">.</span><span class="n">Node</span><span class="p">(</span><span class="n">size_in</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># For max(a, b).
</span>    <span class="n">node_34</span> <span class="o">=</span> <span class="n">nengo</span><span class="p">.</span><span class="n">Node</span><span class="p">(</span><span class="n">size_in</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># For max(c, d).
</span>    <span class="n">net</span><span class="p">.</span><span class="n">otp_node</span> <span class="o">=</span> <span class="n">nengo</span><span class="p">.</span><span class="n">Node</span><span class="p">(</span><span class="n">size_in</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># For max(max(a, b), max(c, d)).
</span>
    <span class="c1">############################################################################
</span>    <span class="c1"># Calculate max(a, b) = (a+b)/2 + |a-b|/2.
</span>    <span class="c1"># Calculate (a+b)/2.
</span>    <span class="n">nengo</span><span class="p">.</span><span class="n">Connection</span><span class="p">(</span><span class="n">net</span><span class="p">.</span><span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">node_12</span><span class="p">,</span> <span class="n">synapse</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">nengo</span><span class="p">.</span><span class="n">Connection</span><span class="p">(</span><span class="n">net</span><span class="p">.</span><span class="n">inputs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">node_12</span><span class="p">,</span> <span class="n">synapse</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">do_max</span><span class="p">:</span>
      <span class="c1"># Calculate |a-b|/2.
</span>      <span class="n">nengo</span><span class="p">.</span><span class="n">Connection</span><span class="p">(</span><span class="n">net</span><span class="p">.</span><span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">ens_12</span><span class="p">,</span> <span class="n">synapse</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span>
      <span class="n">nengo</span><span class="p">.</span><span class="n">Connection</span><span class="p">(</span><span class="n">net</span><span class="p">.</span><span class="n">inputs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">ens_12</span><span class="p">,</span> <span class="n">synapse</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">transform</span><span class="o">=-</span><span class="mi">1</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span>
      <span class="n">nengo</span><span class="p">.</span><span class="n">Connection</span><span class="p">(</span>
          <span class="n">ens_12</span><span class="p">.</span><span class="n">neurons</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">node_12</span><span class="p">,</span> <span class="n">synapse</span><span class="o">=</span><span class="n">synapse</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">radius</span><span class="o">/</span><span class="n">max_rate</span><span class="p">)</span>
      <span class="n">nengo</span><span class="p">.</span><span class="n">Connection</span><span class="p">(</span>
          <span class="n">ens_12</span><span class="p">.</span><span class="n">neurons</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">node_12</span><span class="p">,</span> <span class="n">synapse</span><span class="o">=</span><span class="n">synapse</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">radius</span><span class="o">/</span><span class="n">max_rate</span><span class="p">)</span>
    <span class="c1">############################################################################
</span>
    <span class="c1">############################################################################
</span>    <span class="c1"># Calculate max(c, d) = (c+d)/2 + |c-d|/2.
</span>    <span class="c1"># Calculate (c+d)/2.
</span>    <span class="n">nengo</span><span class="p">.</span><span class="n">Connection</span><span class="p">(</span><span class="n">net</span><span class="p">.</span><span class="n">inputs</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">node_34</span><span class="p">,</span> <span class="n">synapse</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">nengo</span><span class="p">.</span><span class="n">Connection</span><span class="p">(</span><span class="n">net</span><span class="p">.</span><span class="n">inputs</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="n">node_34</span><span class="p">,</span> <span class="n">synapse</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">do_max</span><span class="p">:</span>
      <span class="c1"># Calculate |c-d|/2.
</span>      <span class="n">nengo</span><span class="p">.</span><span class="n">Connection</span><span class="p">(</span><span class="n">net</span><span class="p">.</span><span class="n">inputs</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">ens_34</span><span class="p">,</span> <span class="n">synapse</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span>
      <span class="n">nengo</span><span class="p">.</span><span class="n">Connection</span><span class="p">(</span><span class="n">net</span><span class="p">.</span><span class="n">inputs</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="n">ens_34</span><span class="p">,</span> <span class="n">synapse</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">transform</span><span class="o">=-</span><span class="mi">1</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span>
      <span class="n">nengo</span><span class="p">.</span><span class="n">Connection</span><span class="p">(</span>
          <span class="n">ens_34</span><span class="p">.</span><span class="n">neurons</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">node_34</span><span class="p">,</span> <span class="n">synapse</span><span class="o">=</span><span class="n">synapse</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">radius</span><span class="o">/</span><span class="n">max_rate</span><span class="p">)</span>
      <span class="n">nengo</span><span class="p">.</span><span class="n">Connection</span><span class="p">(</span>
          <span class="n">ens_34</span><span class="p">.</span><span class="n">neurons</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">node_34</span><span class="p">,</span> <span class="n">synapse</span><span class="o">=</span><span class="n">synapse</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">radius</span><span class="o">/</span><span class="n">max_rate</span><span class="p">)</span>
    <span class="c1">############################################################################
</span>
    <span class="c1">############################################################################
</span>    <span class="c1"># Calculate max(a, b, c, d) = max(max(a, b), max(c, d)).
</span>    <span class="c1"># Calculate (node_12 + node_34)/2.
</span>    <span class="n">nengo</span><span class="p">.</span><span class="n">Connection</span><span class="p">(</span><span class="n">node_12</span><span class="p">,</span> <span class="n">net</span><span class="p">.</span><span class="n">otp_node</span><span class="p">,</span> <span class="n">synapse</span><span class="o">=</span><span class="n">synapse</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">nengo</span><span class="p">.</span><span class="n">Connection</span><span class="p">(</span><span class="n">node_34</span><span class="p">,</span> <span class="n">net</span><span class="p">.</span><span class="n">otp_node</span><span class="p">,</span> <span class="n">synapse</span><span class="o">=</span><span class="n">synapse</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">do_max</span><span class="p">:</span>
      <span class="c1"># Calculate |node_12 - node_34|/2.
</span>      <span class="n">nengo</span><span class="p">.</span><span class="n">Connection</span><span class="p">(</span><span class="n">node_12</span><span class="p">,</span> <span class="n">ens_1234</span><span class="p">,</span> <span class="n">synapse</span><span class="o">=</span><span class="n">synapse</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span>
      <span class="n">nengo</span><span class="p">.</span><span class="n">Connection</span><span class="p">(</span><span class="n">node_34</span><span class="p">,</span> <span class="n">ens_1234</span><span class="p">,</span> <span class="n">synapse</span><span class="o">=</span><span class="n">synapse</span><span class="p">,</span> <span class="n">transform</span><span class="o">=-</span><span class="mi">1</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span>
      <span class="n">nengo</span><span class="p">.</span><span class="n">Connection</span><span class="p">(</span><span class="n">ens_1234</span><span class="p">.</span><span class="n">neurons</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">net</span><span class="p">.</span><span class="n">otp_node</span><span class="p">,</span> <span class="n">synapse</span><span class="o">=</span><span class="n">synapse</span><span class="p">,</span>
                       <span class="n">transform</span><span class="o">=</span><span class="n">radius</span><span class="o">/</span><span class="n">max_rate</span><span class="p">)</span>
      <span class="n">nengo</span><span class="p">.</span><span class="n">Connection</span><span class="p">(</span><span class="n">ens_1234</span><span class="p">.</span><span class="n">neurons</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">net</span><span class="p">.</span><span class="n">otp_node</span><span class="p">,</span> <span class="n">synapse</span><span class="o">=</span><span class="n">synapse</span><span class="p">,</span>
                       <span class="n">transform</span><span class="o">=</span><span class="n">radius</span><span class="o">/</span><span class="n">max_rate</span><span class="p">)</span>
    <span class="c1">############################################################################
</span>  <span class="k">return</span> <span class="n">net</span>
</code></pre></div></div>

<p>Following is the plot showing the outputs from the <strong>AVAM</strong> compared to the <code class="language-plaintext highlighter-rouge">True Max U</code>. Note that I have evaluated the <strong>AVAM</strong> Net for multiple <code class="language-plaintext highlighter-rouge">radius</code> values using the same input. Also note that for convenience purposes, I have kept the <code class="language-plaintext highlighter-rouge">radius</code> value same for the neurons in <em>all</em> the <code class="language-plaintext highlighter-rouge">Ensembles</code>.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="avam_net_poc.png" width="500" height="350" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><b> AVAM Net PoC Plot</b></td>
    </tr>
  </tbody>
</table>

<h1 id="quick-analysis-of-the-poc-plots">Quick Analysis of the PoC plots</h1>

<p>As can be seen from the above PoC output plots for both the methods, the <strong>MJOP</strong> Net‚Äôs scaled output closely matches the <code class="language-plaintext highlighter-rouge">True Max U</code> output. In case of the <strong>AVAM</strong> Net too, the outputs for different <code class="language-plaintext highlighter-rouge">radius</code> values closely matches the <code class="language-plaintext highlighter-rouge">True Max U</code> output; this implicitly shows the robustness of the <strong>AVAM</strong> Net w.r.t. the <code class="language-plaintext highlighter-rouge">radius</code> values. In the spirit of pooling methods, I also compared the AveragePooling output with that of the <strong>MJOP</strong> and <strong>AVAM</strong> Nets; as it can be seen, <strong>MJOP</strong> and <strong>AVAM</strong> outputs are higher than the AveragePooled output.</p>

<h1 id="closing-words">Closing Words</h1>

<p>More details on how to adapt the <strong>MJOP</strong> and <strong>AVAM</strong> methods of spiking-MaxPooling in your SNNs can be found in our paper. I have evaluated <strong>MJOP</strong> and <strong>AVAM</strong> on Loihi-\(1\) only, so it will be interesting to see how these methods fare on Loihi-\(2\). I hope this PoC article helped you understand the crux of our paper. Please feel free to post your questions (if any) below!</p>

<hr />
:ET