I"¨<p>In this article, I will demonstrate how to train and test a 2D-CNN based image classification network using the Nengo-DL APIs, by passing the training/test data in batches.</p>

<p>By the end of this article, you will have learned</p>

<ul>
  <li>How to use the <code class="language-plaintext highlighter-rouge">scale_firing_rates</code> parameter while training a Nengo-DL model?</li>
  <li>How to pass the training data in batches to train a model using the Nengo-DL APIs?</li>
  <li>How to pass the test data in batches while inferencing from a trained model using the Nengo-DL APIs?</li>
</ul>

<p>A detailed <a href="https://r-gaurav.github.io/2021/03/07/Spiking-Neural-Nets-for-Image-Classification-in-Nengo-DL.html">previous article</a> on building an SNN and inferencing from it already shows how to pass the test data in batches; if you haven‚Äôt gone through it, I highly recommend doing so. As shown in the linked article, to build an SNN, we first trained an ANN using the TensorFlow (TF) APIs and then converted it to an SNN using the <code class="language-plaintext highlighter-rouge">nengo_dl.Converter()</code> API. In this article though, we will use the Nengo-DL APIs (instead of the TF APIs) to train a Nengo-DL network. This is possible because Nengo-DL uses TF under the hood. By a Nengo-DL network, I still mean an ANN, and <strong>not</strong> an SNN. Directly training an SNN is still an active area of research (<a href="https://arxiv.org/abs/1809.05793">Wu et al.</a>, <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full">Pfeiffer et al.</a>, <a href="https://www.frontiersin.org/articles/10.3389/fnins.2016.00508/full">Lee et al.</a>, <a href="https://www.frontiersin.org/articles/10.3389/fnins.2020.00119/full">Lee at al.</a>, etc.). Bonus: An excellent tutorial on surrogate gradient descent for training SNNs can be be found <a href="https://github.com/fzenke/spytorch">here</a>.</p>

<p>Sometimes, you may want to consider (or make use of) certain network-characteristics of a Nengo-DL network while training a model, e.g. the <code class="language-plaintext highlighter-rouge">scale_firing_rates</code> parameter; which isn‚Äôt natively possible with the TF APIs. Incorporating the <code class="language-plaintext highlighter-rouge">scale_firing_rates</code> parameter while training a network helps in the learning of weights which already account for the increase in neuron firing rates (when scaled later during inference). This may help in the SNNs having sparse activations (experimentally observed), resulting in lesser energy consumption when deployed on a neuromorphic hardware. Note that in the <a href="https://r-gaurav.github.io/2021/03/07/Spiking-Neural-Nets-for-Image-Classification-in-Nengo-DL.html">previous article</a>, <code class="language-plaintext highlighter-rouge">scale_firing_rates</code> parameter was used <strong>only</strong> during the inference phase.</p>

<p>Also, it is possible that your training/test dataset is way too large to fit in its entirety in your GPU‚Äôs RAM. This would necessitate the need of a method to pass the training/test data in batches to the Nengo-DL model; this tutorial shows an example of it. Although the experiment here is done with the MNIST dataset, the method to pass data in batches can be extended to other large datasets as well.</p>

<p>This experiment‚Äôs environment was: <code class="language-plaintext highlighter-rouge">tensorflow-gpu</code> (v2.2.0), <code class="language-plaintext highlighter-rouge">nengo</code> (v3.1.0), and <code class="language-plaintext highlighter-rouge">nengo_dl</code> (v3.4.0); and my machine had a 12GB NVIDIA Tesla P100 GPU. You may have to adjust the values of <code class="language-plaintext highlighter-rouge">train_batch_size</code> and <code class="language-plaintext highlighter-rouge">test_batch_size</code> (in the code to follow) to suit the memory specifications of your machine‚Äôs GPU.</p>

<p>We will</p>

<ul>
  <li>First build a 2D CNN based ANN using the TF APIs</li>
  <li>Then train it using the Nengo-DL APIs by passing the training data in batches</li>
  <li>Then evaluate the trained and converted model i.e. the SNN using the Nengo-DL APIs by passing the test data in batches</li>
</ul>

<p>Let‚Äôs start coding!</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Importing Libraries
</span><span class="kn">import</span> <span class="nn">matplotlib</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">nengo</span>
<span class="kn">import</span> <span class="nn">nengo_dl</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>

<span class="c1"># Set memory growth of GPUs on your system.
</span><span class="n">gpus</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">experimental</span><span class="p">.</span><span class="n">list_physical_devices</span><span class="p">(</span><span class="s">"GPU"</span><span class="p">)</span>
<span class="k">for</span> <span class="n">gpu</span> <span class="ow">in</span> <span class="n">gpus</span><span class="p">:</span>
  <span class="n">tf</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">experimental</span><span class="p">.</span><span class="n">set_memory_growth</span><span class="p">(</span><span class="n">gpu</span><span class="p">,</span> <span class="bp">True</span><span class="p">)</span>

<span class="c1"># Load the MNIST dataset.
</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">),</span> <span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">datasets</span><span class="p">.</span><span class="n">mnist</span><span class="p">.</span><span class="n">load_data</span><span class="p">()</span>

<span class="c1"># Binarize/One-Hot encode the training and test labels.
</span><span class="n">y_train</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">eye</span><span class="p">(</span><span class="mi">10</span><span class="p">)[</span><span class="n">y_train</span><span class="p">].</span><span class="n">squeeze</span><span class="p">()</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">eye</span><span class="p">(</span><span class="mi">10</span><span class="p">)[</span><span class="n">y_test</span><span class="p">].</span><span class="n">squeeze</span><span class="p">()</span>

<span class="c1"># Normalize the dataset.
</span><span class="n">x_train</span> <span class="o">=</span> <span class="n">x_train</span><span class="p">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span> <span class="o">/</span> <span class="mf">127.5</span> <span class="o">-</span> <span class="mi">1</span>
<span class="n">x_test</span> <span class="o">=</span> <span class="n">x_test</span><span class="p">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span> <span class="o">/</span> <span class="mf">127.5</span> <span class="o">-</span> <span class="mi">1</span>
</code></pre></div></div>

<h1 id="building-the-2d-cnn-based-ann">Building the 2D CNN based ANN</h1>

<p>We will reuse the network architecture introduced in the <a href="https://r-gaurav.github.io/2021/03/07/Spiking-Neural-Nets-for-Image-Classification-in-Nengo-DL.html">previous article</a>, except that we will not regularize the kernels in the layers. This is because, if we include the kernel regularizers, it results into creation of ‚Äú<a href="https://www.nengo.ai/nengo-dl/tensor-node">TensorNodes</a>‚Äù in the SNN (obtained after conversion) which aren‚Äôt ‚Äú<a href="https://www.nengo.ai/nengo/frontend-api.html#nengo.Ensemble">Ensemble</a>‚Äù objects in Nengo-DL. Ideally, we should not have any TensorNodes in our SNN, as the TensorNodes (in Nengo-DL) run on GPU and <strong>not</strong> on the neuromorphic hardware. Thus, not really offering any energy efficiency. On the other hand, the Ensembles run on neuromorphic hardware as they are composed of spiking neurons.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">get_2d_cnn_model</span><span class="p">(</span><span class="n">inpt_shape</span><span class="p">):</span>
  <span class="s">"""
  Returns a 2D-CNN model for image classification.

  Args:
    input_shape &lt;tuple&gt;: A tuple of 2D image shape e.g. (28, 28, 1)
  """</span>
  <span class="k">def</span> <span class="nf">_get_cnn_block</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">num_filters</span><span class="p">,</span> <span class="n">layer_objs_lst</span><span class="p">):</span>
    <span class="n">conv</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Conv2D</span><span class="p">(</span>
        <span class="n">num_filters</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s">"same"</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">"relu"</span><span class="p">,</span>
        <span class="n">kernel_initializer</span><span class="o">=</span><span class="s">"he_uniform"</span><span class="p">)(</span><span class="n">layer</span><span class="p">)</span>
    <span class="n">avg_pool</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">AveragePooling2D</span><span class="p">()(</span><span class="n">conv</span><span class="p">)</span>

    <span class="n">layer_objs_lst</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">conv</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">avg_pool</span>

  <span class="n">layer_objs_lst</span> <span class="o">=</span> <span class="p">[]</span> <span class="c1"># To store the layer objects to probe later in Nengo-DL
</span>
  <span class="n">inpt_layer</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="n">inpt_shape</span><span class="p">)</span>
  <span class="n">layer_objs_lst</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">inpt_layer</span><span class="p">)</span>

  <span class="n">layer</span> <span class="o">=</span> <span class="n">_get_cnn_block</span><span class="p">(</span><span class="n">inpt_layer</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="n">layer_objs_lst</span><span class="p">)</span>
  <span class="n">layer</span> <span class="o">=</span> <span class="n">_get_cnn_block</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">layer_objs_lst</span><span class="p">)</span>
  <span class="n">layer</span> <span class="o">=</span> <span class="n">_get_cnn_block</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="n">layer_objs_lst</span><span class="p">)</span>

  <span class="n">flat</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Flatten</span><span class="p">()(</span><span class="n">layer</span><span class="p">)</span>

  <span class="n">dense</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span>
      <span class="mi">512</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">"relu"</span><span class="p">,</span> <span class="n">kernel_initializer</span><span class="o">=</span><span class="s">"he_uniform"</span><span class="p">)(</span><span class="n">flat</span><span class="p">)</span>
  <span class="n">layer_objs_lst</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">dense</span><span class="p">)</span>

  <span class="n">output_layer</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span>
      <span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">"softmax"</span><span class="p">,</span> <span class="n">kernel_initializer</span><span class="o">=</span><span class="s">"he_uniform"</span><span class="p">)(</span><span class="n">dense</span><span class="p">)</span>
  <span class="n">layer_objs_lst</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">output_layer</span><span class="p">)</span>

  <span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">inpt_layer</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">output_layer</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">model</span><span class="p">,</span> <span class="n">layer_objs_lst</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">get_2d_cnn_model</span><span class="p">((</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">model</span><span class="p">.</span><span class="n">summary</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Model: "model"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         [(None, 28, 28, 1)]       0
_________________________________________________________________
conv2d (Conv2D)              (None, 28, 28, 32)        320
_________________________________________________________________
average_pooling2d (AveragePo (None, 14, 14, 32)        0
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 14, 14, 64)        18496
_________________________________________________________________
average_pooling2d_1 (Average (None, 7, 7, 64)          0
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 7, 7, 128)         73856
_________________________________________________________________
average_pooling2d_2 (Average (None, 3, 3, 128)         0
_________________________________________________________________
flatten (Flatten)            (None, 1152)              0
_________________________________________________________________
dense (Dense)                (None, 512)               590336
_________________________________________________________________
dense_1 (Dense)              (None, 10)                5130
=================================================================
Total params: 688,138
Trainable params: 688,138
Non-trainable params: 0
_________________________________________________________________
</code></pre></div></div>

<p>Let us print the network‚Äôs layers‚Äô name, as well as their output shapes. This will be helpful while curating the training dataset in batches, where we need to mention the layers‚Äô names as keys against matrices as values (defined later) in a dictionary.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="s">"2D-CNN model's layers' name..."</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"-"</span><span class="o">*</span><span class="mi">50</span><span class="p">)</span>
<span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">layers</span><span class="p">):</span>
  <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Layer ID </span><span class="si">{</span><span class="n">index</span><span class="si">}</span><span class="s"> || and Layer name: </span><span class="si">{</span><span class="n">layer</span><span class="p">.</span><span class="n">name</span><span class="si">}</span><span class="s"> || and output_shape: </span><span class="si">{</span><span class="n">layer</span><span class="p">.</span><span class="n">output_shape</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"-"</span><span class="o">*</span><span class="mi">50</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>2D-CNN model's layers' name...
--------------------------------------------------
Layer ID 0 || and Layer name: input_1 || and output_shape: [(None, 28, 28, 1)]
Layer ID 1 || and Layer name: conv2d || and output_shape: (None, 28, 28, 32)
Layer ID 2 || and Layer name: average_pooling2d || and output_shape: (None, 14, 14, 32)
Layer ID 3 || and Layer name: conv2d_1 || and output_shape: (None, 14, 14, 64)
Layer ID 4 || and Layer name: average_pooling2d_1 || and output_shape: (None, 7, 7, 64)
Layer ID 5 || and Layer name: conv2d_2 || and output_shape: (None, 7, 7, 128)
Layer ID 6 || and Layer name: average_pooling2d_2 || and output_shape: (None, 3, 3, 128)
Layer ID 7 || and Layer name: flatten || and output_shape: (None, 1152)
Layer ID 8 || and Layer name: dense || and output_shape: (None, 512)
Layer ID 9 || and Layer name: dense_1 || and output_shape: (None, 10)
--------------------------------------------------
</code></pre></div></div>

<h1 id="creating-the-nengo-dl-network">Creating the Nengo-DL network</h1>

<p>The following code creates and returns a Nengo-DL network; either an ANN or an SNN depending on value of <code class="language-plaintext highlighter-rouge">mode</code> (in the function below). Note that while training (i.e. <code class="language-plaintext highlighter-rouge">mode</code> = ‚Äútrain‚Äù), we do <strong>not</strong> replace the ReLU neurons with spiking neurons in the TF model  while calling the <code class="language-plaintext highlighter-rouge">nengo_dl.Converter()</code> API, hence, an ANN is returned. However, while inferencing (i.e. <code class="language-plaintext highlighter-rouge">mode</code> = ‚Äútest‚Äù), we replace the ReLU neurons with spiking neurons in the <code class="language-plaintext highlighter-rouge">nengo_dl.Converter()</code> API, hence, an SNN is returned.</p>

<p>Note that the <code class="language-plaintext highlighter-rouge">scale_firing_rates</code> parameter is assigned a value in both the <code class="language-plaintext highlighter-rouge">mode</code>s.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">get_nengo_dl_model</span><span class="p">(</span><span class="n">sfr</span><span class="p">,</span> <span class="n">mode</span><span class="p">):</span>
  <span class="s">"""
  Returns a Nengo-DL model for image classification.

  Args:
    sfr &lt;int&gt;: Value for the `scale_firing_rates` parameters.
    mode &lt;str&gt;: One of "train" or "test".
  """</span>
  <span class="k">if</span> <span class="n">mode</span> <span class="o">!=</span> <span class="s">"train"</span> <span class="ow">and</span> <span class="n">mode</span> <span class="o">!=</span> <span class="s">"test"</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Wrong mode: </span><span class="si">{</span><span class="n">mode</span><span class="si">}</span><span class="s"> while getting Nengo-DL model!! Exiting..."</span><span class="p">)</span>
    <span class="n">sys</span><span class="p">.</span><span class="nb">exit</span><span class="p">()</span>

  <span class="c1"># MNIST has image shape: (28, 28, 1).
</span>  <span class="n">model</span><span class="p">,</span> <span class="n">layer_objs_lst</span> <span class="o">=</span> <span class="n">get_2d_cnn_model</span><span class="p">((</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

  <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="s">"train"</span><span class="p">:</span>
    <span class="c1"># Create the Nengo-DL network - a Nengo-DL wrapper over ANN here.
</span>    <span class="c1"># Note that we aren't replacing the ReLU neurons.
</span>    <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">ndl_model</span> <span class="o">=</span> <span class="n">nengo_dl</span><span class="p">.</span><span class="n">Converter</span><span class="p">(</span>
        <span class="n">model</span><span class="p">,</span>
        <span class="n">scale_firing_rates</span><span class="o">=</span><span class="n">sfr</span>
    <span class="p">)</span>
  <span class="k">elif</span> <span class="n">mode</span> <span class="o">==</span> <span class="s">"test"</span><span class="p">:</span>
    <span class="c1"># Create the Nengo-DL network. Converting the ANN to SNN here.
</span>    <span class="c1"># Note that we are replacing the ReLU neurons with spiking ReLU.
</span>    <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">ndl_model</span> <span class="o">=</span> <span class="n">nengo_dl</span><span class="p">.</span><span class="n">Converter</span><span class="p">(</span>
        <span class="n">model</span><span class="p">,</span>
        <span class="n">swap_activations</span><span class="o">=</span><span class="p">{</span>
            <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">activations</span><span class="p">.</span><span class="n">relu</span><span class="p">:</span> <span class="n">nengo</span><span class="p">.</span><span class="n">SpikingRectifiedLinear</span><span class="p">()},</span>
        <span class="n">scale_firing_rates</span><span class="o">=</span><span class="n">sfr</span><span class="p">,</span>
        <span class="n">synapse</span><span class="o">=</span><span class="mf">0.005</span>
    <span class="p">)</span>

  <span class="c1"># Get the probes for Input, first Conv, and the Output layers.
</span>  <span class="n">ndl_probes</span> <span class="o">=</span> <span class="p">[</span><span class="n">ndl_model</span><span class="p">.</span><span class="n">inputs</span><span class="p">[</span><span class="n">layer_objs_lst</span><span class="p">[</span><span class="mi">0</span><span class="p">]]]</span> <span class="c1"># Input layer probe.
</span>
  <span class="k">with</span> <span class="n">ndl_model</span><span class="p">.</span><span class="n">net</span><span class="p">:</span>
    <span class="n">nengo_dl</span><span class="p">.</span><span class="n">configure_settings</span><span class="p">(</span><span class="n">stateful</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span> <span class="c1"># Optimize simulation speed.
</span>    <span class="c1"># Probe for the first Conv layer.
</span>    <span class="n">first_conv_probe</span> <span class="o">=</span> <span class="n">nengo</span><span class="p">.</span><span class="n">Probe</span><span class="p">(</span><span class="n">ndl_model</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="n">layer_objs_lst</span><span class="p">[</span><span class="mi">1</span><span class="p">]])</span>
    <span class="n">ndl_probes</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">first_conv_probe</span><span class="p">)</span>
    <span class="c1"># Probe for penultimate dense layer.
</span>    <span class="n">penltmt_dense_probe</span> <span class="o">=</span> <span class="n">nengo</span><span class="p">.</span><span class="n">Probe</span><span class="p">(</span><span class="n">ndl_model</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="n">layer_objs_lst</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]])</span>
    <span class="n">ndl_probes</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">penltmt_dense_probe</span><span class="p">)</span>

  <span class="n">ndl_probes</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">ndl_model</span><span class="p">.</span><span class="n">outputs</span><span class="p">[</span><span class="n">layer_objs_lst</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]])</span> <span class="c1"># Output layer probe.
</span>
  <span class="k">return</span> <span class="n">ndl_model</span><span class="p">,</span> <span class="n">ndl_probes</span>
</code></pre></div></div>

<h1 id="curating-the-dataset">Curating the dataset</h1>

<p>We use the MNIST dataset for our experiments. For such a small dataset, we really don‚Äôt need to create batches of training/test data. However, the code shown here to create batches and pass it to the Nengo-DL APIs can be extended to other large datasets as well.</p>

<h2 id="for-training">For Training</h2>

<p>While creating the batches of training data, we need to create two dictionaries: one for the input data (i.e. training images), and another for the output data (i.e. training labels). In the input dictionary, the keys are the layers‚Äô name. The key with the model‚Äôs input layer‚Äôs name has training images‚Äô data as value, and another key with the name ‚Äún_steps‚Äù has a matrix of ones as value (of shape: <code class="language-plaintext highlighter-rouge">(batch_size, 1)</code>) - we need to present the training images for only one time-step. These two keys are important and should be mentioned in the input dictionary. In case the model has layers with <code class="language-plaintext highlighter-rouge">use_bias=True</code> (which is ‚ÄúTrue‚Äù by default in the TF APIs for layers), we need to append those layers‚Äô name with ‚Äú.0.bias‚Äù and mention them as keys against matrices of ones as values (in fact, matrix values can be any, I chose ones). Those matrices are of shape <code class="language-plaintext highlighter-rouge">(batch_size, number_of_channels, 1)</code> for Conv layers, and of shape <code class="language-plaintext highlighter-rouge">(batch_size, number_of_neurons, 1)</code> for Dense layers. Note that such matrices are defined only for the layers with neurons.</p>

<h2 id="for-inference">For Inference</h2>

<p>While creating the batches of test data for inference, it‚Äôs quite simple; we just create it as usual and then return it. We don‚Äôt need to create any dictionaries with custom values.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">get_batches_of_dataset</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">n_steps</span><span class="p">,</span> <span class="n">mode</span><span class="p">,</span> <span class="n">layers</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
  <span class="s">"""
  Returns batches of NengoDL compatible training or test data.

  Args:
    batch_size &lt;int&gt;: Batch size of the training or test data.
    n_steps &lt;int&gt;: Number of time-steps an image is presented to the network.
    mode &lt;str&gt;: One of "train" or "test".
    layers &lt;[]&gt;: List of TensorFlow type layers.
  """</span>
  <span class="k">if</span> <span class="n">mode</span> <span class="o">!=</span> <span class="s">"train"</span> <span class="ow">and</span> <span class="n">mode</span> <span class="o">!=</span> <span class="s">"test"</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Wrong mode: </span><span class="si">{</span><span class="n">mode</span><span class="si">}</span><span class="s"> while curating dataset!! Exiting..."</span><span class="p">)</span>
    <span class="n">sys</span><span class="p">.</span><span class="nb">exit</span><span class="p">()</span>

  <span class="n">num_train_imgs</span><span class="p">,</span> <span class="n">num_test_imgs</span> <span class="o">=</span> <span class="n">x_train</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x_test</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
  <span class="n">reshaped_x_train</span> <span class="o">=</span> <span class="n">x_train</span><span class="p">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">num_train_imgs</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
  <span class="n">reshaped_x_test</span> <span class="o">=</span> <span class="n">x_test</span><span class="p">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">num_test_imgs</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
  <span class="n">reshaped_y_train</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">num_train_imgs</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>

  <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="s">"test"</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">start</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">num_test_imgs</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
      <span class="c1"># Nengo-DL model complains if the batch_size of data is lesser than actual.
</span>      <span class="k">if</span> <span class="n">start</span><span class="o">+</span><span class="n">batch_size</span> <span class="o">&gt;</span> <span class="n">num_test_imgs</span><span class="p">:</span>
        <span class="k">continue</span>

      <span class="c1"># Tile the images, i.e. repeat each image `n_steps` number of times.
</span>      <span class="n">tiled_x_test</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">tile</span><span class="p">(</span>
          <span class="n">reshaped_x_test</span><span class="p">[</span><span class="n">start</span> <span class="p">:</span> <span class="n">start</span><span class="o">+</span><span class="n">batch_size</span><span class="p">],</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_steps</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
      <span class="k">yield</span> <span class="p">(</span><span class="n">tiled_x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">[</span><span class="n">start</span> <span class="p">:</span> <span class="n">start</span><span class="o">+</span><span class="n">batch_size</span><span class="p">])</span>

  <span class="k">elif</span> <span class="n">mode</span> <span class="o">==</span> <span class="s">"train"</span><span class="p">:</span>
    <span class="c1"># During training, since we train a non-spiking network,
</span>    <span class="c1"># we present the images only once.
</span>    <span class="k">assert</span> <span class="n">n_steps</span> <span class="o">==</span> <span class="mi">1</span>
    <span class="k">for</span> <span class="n">start</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">num_train_imgs</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
      <span class="n">tiled_x_train</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">tile</span><span class="p">(</span> <span class="c1"># n_steps = 1 here.
</span>        <span class="n">reshaped_x_train</span><span class="p">[</span><span class="n">start</span> <span class="p">:</span> <span class="n">start</span><span class="o">+</span><span class="n">batch_size</span><span class="p">],</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_steps</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

      <span class="c1"># Note that in the `input_dict` below there's no bias value mentioned for
</span>      <span class="c1"># the AveragePooling2D and Flatten Layers, as these layers have no neurons.
</span>      <span class="n">input_dict</span> <span class="o">=</span> <span class="p">{</span>
        <span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">name</span><span class="p">:</span> <span class="n">tiled_x_train</span><span class="p">,</span> <span class="c1"># Layer 0 is the input layer.
</span>        <span class="c1"># Next Conv layer has 32 channels.
</span>        <span class="n">layers</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="n">name</span><span class="o">+</span><span class="s">".0.bias"</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ones</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">int32</span><span class="p">),</span>
        <span class="c1"># Next Conv layer has 64 channels.
</span>        <span class="n">layers</span><span class="p">[</span><span class="mi">3</span><span class="p">].</span><span class="n">name</span><span class="o">+</span><span class="s">".0.bias"</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ones</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">int32</span><span class="p">),</span>
        <span class="c1"># Next Conv layer has 128 channels.
</span>        <span class="n">layers</span><span class="p">[</span><span class="mi">5</span><span class="p">].</span><span class="n">name</span><span class="o">+</span><span class="s">".0.bias"</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ones</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">int32</span><span class="p">),</span>
        <span class="c1"># Next Dense layer has 512 neurons.
</span>        <span class="n">layers</span><span class="p">[</span><span class="mi">8</span><span class="p">].</span><span class="n">name</span><span class="o">+</span><span class="s">".0.bias"</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ones</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">int32</span><span class="p">),</span>
        <span class="c1"># Next Dense layer has 10 neurons.
</span>        <span class="n">layers</span><span class="p">[</span><span class="mi">9</span><span class="p">].</span><span class="n">name</span><span class="o">+</span><span class="s">".0.bias"</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ones</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">int32</span><span class="p">),</span>
        <span class="c1"># Mention the value of n_steps parameter.
</span>        <span class="s">"n_steps"</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ones</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">int32</span><span class="p">)</span>
      <span class="p">}</span>
      <span class="n">output_dict</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s">"probe"</span><span class="p">:</span> <span class="n">reshaped_y_train</span><span class="p">[</span><span class="n">start</span> <span class="p">:</span> <span class="n">start</span><span class="o">+</span><span class="n">batch_size</span><span class="p">]</span>
      <span class="p">}</span>

      <span class="k">yield</span> <span class="p">(</span><span class="n">input_dict</span><span class="p">,</span> <span class="n">output_dict</span><span class="p">)</span>
</code></pre></div></div>

<h1 id="training-the-nengo-dl-network">Training the Nengo-DL Network</h1>

<p>Next, we obtain the Nengo-DL network i.e. the ANN for training and train it via Nengo-DL APIs in a batchwise manner. For that, we can simply pass the training data generator (obtained from the <code class="language-plaintext highlighter-rouge">get_batches_of_dataset()</code> function above) to the <code class="language-plaintext highlighter-rouge">fit()</code> function of the Nengo-DL simulator object holding the network. We will then iteratively call the <code class="language-plaintext highlighter-rouge">fit()</code> function for a certain number of training epochs. Note the similarity of this Nengo-DL <code class="language-plaintext highlighter-rouge">fit()</code> function‚Äôs interface with that of the TF <code class="language-plaintext highlighter-rouge">fit()</code> function. After training the network, we will save the trained parameters, to be loaded later for inferencing.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sfr</span> <span class="o">=</span> <span class="mi">25</span>
<span class="n">train_batch_size</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">num_train_imgs</span> <span class="o">=</span> <span class="n">x_train</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">ndl_model</span><span class="p">,</span> <span class="n">ndl_model_probes</span> <span class="o">=</span> <span class="n">get_nengo_dl_model</span><span class="p">(</span><span class="n">sfr</span><span class="p">,</span> <span class="s">"train"</span><span class="p">)</span>

<span class="c1"># Train the Nengo-DL Model.
</span><span class="k">with</span> <span class="n">nengo_dl</span><span class="p">.</span><span class="n">Simulator</span><span class="p">(</span><span class="n">ndl_model</span><span class="p">.</span><span class="n">net</span><span class="p">,</span> <span class="n">minibatch_size</span><span class="o">=</span><span class="n">train_batch_size</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                        <span class="n">progress_bar</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span> <span class="k">as</span> <span class="n">ndl_sim</span><span class="p">:</span>
  <span class="c1"># Define the loss function applied at the output layer.
</span>  <span class="n">losses</span> <span class="o">=</span> <span class="p">{</span>
    <span class="n">ndl_model_probes</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span> <span class="n">tf</span><span class="p">.</span><span class="n">losses</span><span class="p">.</span><span class="n">CategoricalCrossentropy</span><span class="p">()</span>
  <span class="p">}</span>
  <span class="c1"># Compile the Nengo-DL model.
</span>  <span class="n">ndl_sim</span><span class="p">.</span><span class="nb">compile</span><span class="p">(</span>
    <span class="n">optimizer</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">optimizers</span><span class="p">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">),</span>
    <span class="n">loss</span><span class="o">=</span><span class="n">losses</span><span class="p">,</span>
    <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s">"accuracy"</span><span class="p">]</span>
  <span class="p">)</span>
  <span class="n">ndl_model_layers</span> <span class="o">=</span> <span class="n">ndl_model</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">layers</span>

  <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">8</span><span class="p">):</span> <span class="c1"># Train for 8 epochs.
</span>    <span class="c1"># Set n_steps=1 and mode="train" in the statment below for training.
</span>    <span class="n">train_batches</span> <span class="o">=</span> <span class="n">get_batches_of_dataset</span><span class="p">(</span>
        <span class="n">train_batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="s">"train"</span><span class="p">,</span> <span class="n">ndl_model_layers</span><span class="p">)</span>
    <span class="n">ndl_sim</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_batches</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                <span class="n">steps_per_epoch</span><span class="o">=</span><span class="n">num_train_imgs</span><span class="o">//</span><span class="n">train_batch_size</span><span class="p">)</span>

  <span class="c1"># Save the trained network parameters.
</span>  <span class="n">ndl_sim</span><span class="p">.</span><span class="n">save_params</span><span class="p">(</span><span class="s">"nengo_dl_trained_model_params"</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">"All Epochs Done! Training Completed."</span><span class="p">)</span>
</code></pre></div></div>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>/home/rgaurav/miniconda3/envs/latest-nengo-tf/lib/python3.7/site-packages/nengo_dl/converter.py:588: UserWarning: Activation type &lt;function softmax at 0x2b65ab157d40&gt; does not have a native Nengo equivalent; falling back to a TensorNode
  "falling back to a TensorNode" % activation
/home/rgaurav/miniconda3/envs/latest-nengo-tf/lib/python3.7/site-packages/nengo_dl/simulator.py:1773: UserWarning: Number of elements (1) in ['str'] does not match number of Probes (3); consider using an explicit input dictionary in this case, so that the assignment of data to objects is unambiguous.
  len(objects),
2021-12-31 20:28:35.000233: W tensorflow/stream_executor/gpu/asm_compiler.cc:81] Running ptxas --version returned 256
2021-12-31 20:28:35.052992: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] Internal: ptxas exited with non-zero error code 256, output:
Relying on driver to perform ptx compilation.
Modify $PATH to customize ptxas location.
This message will be only logged once.


600/600 [==============================] - 6s 11ms/step - loss: 0.2237 - probe_loss: 0.2237 - probe_accuracy: 0.9338
600/600 [==============================] - 6s 10ms/step - loss: 0.0543 - probe_loss: 0.0543 - probe_accuracy: 0.9838
600/600 [==============================] - 6s 10ms/step - loss: 0.0366 - probe_loss: 0.0366 - probe_accuracy: 0.9893
600/600 [==============================] - 6s 10ms/step - loss: 0.0275 - probe_loss: 0.0275 - probe_accuracy: 0.9922
600/600 [==============================] - 6s 10ms/step - loss: 0.0224 - probe_loss: 0.0224 - probe_accuracy: 0.9933
600/600 [==============================] - 6s 10ms/step - loss: 0.0177 - probe_loss: 0.0177 - probe_accuracy: 0.9947
600/600 [==============================] - 6s 10ms/step - loss: 0.0139 - probe_loss: 0.0139 - probe_accuracy: 0.9956
600/600 [==============================] - 6s 10ms/step - loss: 0.0137 - probe_loss: 0.0137 - probe_accuracy: 0.9956
All Epochs Done! Training Completed.
</code></pre></div></div>

<p>After training for \(8\) epochs, I achieved a training accuracy of \(99.56\%\). You might get a similar accuracy score.</p>

<h1 id="inferencing-from-the-nengo-dl-network">Inferencing from the Nengo-DL Network</h1>

<p>Now that we have trained and saved the weights of the Nengo-DL network i.e. the ANN, let us convert it to an SNN. We can do this by simply replacing the ReLU neurons in the ANN with spiking ones; note that we also mention the <code class="language-plaintext highlighter-rouge">synapse</code> value as well as the <code class="language-plaintext highlighter-rouge">scale_firing_rates</code> value (in the <code class="language-plaintext highlighter-rouge">nengo_dl.Converter()</code> API in the <code class="language-plaintext highlighter-rouge">get_nengo_dl_model()</code> function) while converting to an SNN. We can then load the trained parameters into the SNN and predict on the test data passed in batches, and simultaneously collect the spikes and calculate accuracy. Note that to obtain a label for a test image, we need to execute the network for a certain number of time-steps for each image; this is taken care of while creating the test data - recollect tiling each test image. Here we execute the network for <code class="language-plaintext highlighter-rouge">n_steps</code> = \(60\) time-steps.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">test_batch_size</span><span class="p">,</span> <span class="n">test_acc</span><span class="p">,</span> <span class="n">n_steps</span> <span class="o">=</span> <span class="mi">200</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">60</span>
<span class="n">num_test_imgs</span> <span class="o">=</span> <span class="n">x_test</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">collect_spikes_output</span> <span class="o">=</span> <span class="bp">True</span>
<span class="n">ndl_mdl_spikes</span> <span class="o">=</span> <span class="p">[]</span> <span class="c1"># To store the spike outputs of the first Conv layer and the
</span>                    <span class="c1"># penultimate dense layer whose probes we defined earlier.
</span><span class="n">ndl_model</span><span class="p">,</span> <span class="n">ndl_model_probes</span> <span class="o">=</span> <span class="n">get_nengo_dl_model</span><span class="p">(</span><span class="n">sfr</span><span class="p">,</span> <span class="s">"test"</span><span class="p">)</span>
<span class="c1"># Set n_steps=60 and mode="test" in the statment below for training.
</span><span class="n">test_batches</span> <span class="o">=</span> <span class="n">get_batches_of_dataset</span><span class="p">(</span><span class="n">test_batch_size</span><span class="p">,</span> <span class="n">n_steps</span><span class="o">=</span><span class="n">n_steps</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s">"test"</span><span class="p">)</span>

<span class="c1"># Run the simulation for inference.
</span><span class="k">with</span> <span class="n">nengo_dl</span><span class="p">.</span><span class="n">Simulator</span><span class="p">(</span><span class="n">ndl_model</span><span class="p">.</span><span class="n">net</span><span class="p">,</span> <span class="n">minibatch_size</span><span class="o">=</span><span class="n">test_batch_size</span><span class="p">,</span>
                        <span class="n">progress_bar</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span> <span class="k">as</span> <span class="n">ndl_sim</span><span class="p">:</span>
  <span class="c1"># Load the trained weights.
</span>  <span class="n">ndl_sim</span><span class="p">.</span><span class="n">load_params</span><span class="p">(</span><span class="s">"nengo_dl_trained_model_params"</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">test_batches</span><span class="p">:</span>
    <span class="c1"># Pass the test data to the input layer, and predict on it.
</span>    <span class="n">sim_data</span> <span class="o">=</span> <span class="n">ndl_sim</span><span class="p">.</span><span class="n">predict_on_batch</span><span class="p">({</span><span class="n">ndl_model_probes</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span> <span class="n">batch</span><span class="p">[</span><span class="mi">0</span><span class="p">]})</span>
    <span class="c1"># Obtain predicted labels from last layer, and match it to the true labels.
</span>    <span class="k">for</span> <span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">sim_data</span><span class="p">[</span><span class="n">ndl_model_probes</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]]):</span>
      <span class="k">if</span> <span class="n">np</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">y_true</span><span class="p">)</span> <span class="o">==</span> <span class="n">np</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">y_pred</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]):</span>
        <span class="n">test_acc</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="c1"># Collect the spikes if required.
</span>    <span class="k">if</span> <span class="n">collect_spikes_output</span><span class="p">:</span>
      <span class="c1"># Collecting spikes for each image in the first batch.
</span>      <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">test_batch_size</span><span class="p">):</span>
        <span class="n">ndl_mdl_spikes</span><span class="p">.</span><span class="n">append</span><span class="p">({</span>
          <span class="n">ndl_model_probes</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="n">obj</span><span class="p">.</span><span class="n">ensemble</span><span class="p">.</span><span class="n">label</span><span class="p">:</span> <span class="n">sim_data</span><span class="p">[</span><span class="n">ndl_model_probes</span><span class="p">[</span><span class="mi">1</span><span class="p">]][</span><span class="n">i</span><span class="p">],</span>
          <span class="n">ndl_model_probes</span><span class="p">[</span><span class="mi">2</span><span class="p">].</span><span class="n">obj</span><span class="p">.</span><span class="n">ensemble</span><span class="p">.</span><span class="n">label</span><span class="p">:</span> <span class="n">sim_data</span><span class="p">[</span><span class="n">ndl_model_probes</span><span class="p">[</span><span class="mi">2</span><span class="p">]][</span><span class="n">i</span><span class="p">]</span>
        <span class="p">})</span>
      <span class="c1"># Not collecting the spikes for rest of the batches to save memory.
</span>      <span class="n">collect_spikes_output</span> <span class="o">=</span> <span class="bp">False</span>

<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Accuracy of the SNN over MNIST test images: </span><span class="si">{</span><span class="n">test_acc</span><span class="o">*</span><span class="mi">100</span><span class="o">/</span><span class="n">num_test_imgs</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Accuracy of the SNN over MNIST test images: 97.93
</code></pre></div></div>

<p>With our SNN, we achieve a test accuracy score of \(97.93\%\); you may obtain a similar score.</p>

<h1 id="plotting-spikes">Plotting Spikes</h1>

<p>We reuse the code from the <a href="https://r-gaurav.github.io/2021/03/07/Spiking-Neural-Nets-for-Image-Classification-in-Nengo-DL.html">previous article</a> to plot the spikes obtained from the first Conv layer and the penultimate Dense layer of the SNN. In both the plots, note the sparsity of the spiking activity.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">plot_spikes</span><span class="p">(</span><span class="n">probe</span><span class="p">,</span> <span class="n">test_data_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">num_neurons</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">dt</span><span class="o">=</span><span class="mf">0.001</span><span class="p">):</span>
  <span class="s">"""
  Plots the spikes of the layer corresponding to the `probe`.

  Args:
    probe &lt;nengo.probe.Probe&gt;: The probe object of the layer whose spikes are to
                               be plotted.
    test_data_idx &lt;int&gt;: Test image's index for which spikes were generated.
    num_neurons &lt;int&gt;: Number of random neurons for which spikes are to be plotted.
    dt &lt;int&gt;: The duration of each timestep. Nengo-DL's default duration is 0.001s.
  """</span>
  <span class="n">lyr_name</span> <span class="o">=</span> <span class="n">probe</span><span class="p">.</span><span class="n">obj</span><span class="p">.</span><span class="n">ensemble</span><span class="p">.</span><span class="n">label</span>
  <span class="n">spikes_matrix</span> <span class="o">=</span> <span class="n">ndl_mdl_spikes</span><span class="p">[</span><span class="n">test_data_idx</span><span class="p">][</span><span class="n">lyr_name</span><span class="p">]</span> <span class="o">*</span> <span class="n">sfr</span> <span class="o">*</span> <span class="n">dt</span>
  <span class="n">neurons</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">choice</span><span class="p">(</span><span class="n">spikes_matrix</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">num_neurons</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
  <span class="n">spikes_matrix</span> <span class="o">=</span> <span class="n">spikes_matrix</span><span class="p">[:,</span> <span class="n">neurons</span><span class="p">]</span>

  <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">12</span><span class="p">),</span> <span class="n">facecolor</span><span class="o">=</span><span class="s">"#00FFFF"</span><span class="p">)</span>
  <span class="n">color</span> <span class="o">=</span> <span class="n">matplotlib</span><span class="p">.</span><span class="n">cm</span><span class="p">.</span><span class="n">get_cmap</span><span class="p">(</span><span class="s">'tab10'</span><span class="p">)(</span><span class="mi">0</span><span class="p">)</span>
  <span class="n">timesteps</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_steps</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_neurons</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">timesteps</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">where</span><span class="p">(</span><span class="n">spikes_matrix</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">)]:</span>
      <span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">([</span><span class="n">t</span><span class="p">,</span> <span class="n">t</span><span class="p">],</span> <span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">i</span><span class="o">+</span><span class="mf">1.5</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">)</span>

  <span class="n">ax</span><span class="p">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">num_neurons</span><span class="o">+</span><span class="mf">0.5</span><span class="p">)</span>
  <span class="n">ax</span><span class="p">.</span><span class="n">set_yticks</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_neurons</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">num_neurons</span><span class="o">/</span><span class="mi">50</span><span class="p">)))))</span>
  <span class="n">ax</span><span class="p">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_steps</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">)))</span>
  <span class="n">ax</span><span class="p">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">"Neuron Index"</span><span class="p">)</span>
  <span class="n">ax</span><span class="p">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">"Time in $ms$"</span><span class="p">)</span>
  <span class="n">ax</span><span class="p">.</span><span class="n">set_title</span><span class="p">(</span><span class="s">"Layer: %s"</span> <span class="o">%</span> <span class="n">lyr_name</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="spike-plot-of-the-first-conv-layer">Spike Plot of the first Conv layer</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plot_spikes</span><span class="p">(</span><span class="n">ndl_model_probes</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="c1"># First Conv Layer.
</span></code></pre></div></div>

<p><img src="output_20_0.png" alt="png" /></p>

<h2 id="spike-plot-of-the-penultimate-dense-layer">Spike Plot of the penultimate Dense layer</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plot_spikes</span><span class="p">(</span><span class="n">ndl_model_probes</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span> <span class="c1"># Penultimate Dense Layer.
</span></code></pre></div></div>

<p><img src="output_22_0.png" alt="png" /></p>

<h1 id="analysis">Analysis</h1>

<p>In the <a href="https://r-gaurav.github.io/2021/03/07/Spiking-Neural-Nets-for-Image-Classification-in-Nengo-DL.html">previous article</a>, we did <strong>not</strong> use the Nengo-DL APIs to train the ANN, rather used the TF APIs and then converted it to an SNN; while conversion for inference, we also scaled the neuron firing rates i.e. assigned a value to the <code class="language-plaintext highlighter-rouge">scale_firing_rates</code> parameter. We also saw that the first Conv layer‚Äôs spike plot was quite dense, although that was obtained with the CIFAR10 dataset. In case you run the <a href="https://r-gaurav.github.io/2021/03/07/Spiking-Neural-Nets-for-Image-Classification-in-Nengo-DL.html">previous article</a> with the MNIST dataset, you will observe a similar dense spiking activity. Recollect that sparse spiking activity (and <strong>not</strong> dense) is desirable due to it consuming lesser energy.</p>

<p>In this article we observe that the spiking activity of the first Conv layer is quite sparse! Spiking activity plots of the deeper layers would be sparse as well, because they receive inputs from the previous layers. The sparsity in spiking activity is due to training the ANN using the Nengo-DL APIs in cognizance of the <code class="language-plaintext highlighter-rouge">scale_firing_rates</code> parameter (also, normalizing the dataset adds to the sparsity). In this case, Nengo-DL APIs consider the conversion of ANNs to SNNs beforehand and optimize the training process to create SNNs which better account for the dynamics of the spiking neurons, thus producing sparse activations. Although, we did not tune the <code class="language-plaintext highlighter-rouge">scale_firing_rates</code> parameter in this article, feel free to try out other values; in fact try out different values for training and inference. It is possible that if you do not set a value of the <code class="language-plaintext highlighter-rouge">scale_firing_rates</code> parameter while creating the Nengo-DL model for training, it would still produce a reasonably performing SNN (upon conversion) with sparse activations, however, a tuning might be necessary for a desired performance. Note that with the increase in the <code class="language-plaintext highlighter-rouge">scale_firing_rates</code> parameter value‚Ä¶ although the test accuracy score would improve, spike activations may get denser. Also note that if the <code class="language-plaintext highlighter-rouge">scale_firing_rates</code> parameter isn‚Äôt mentioned in the <code class="language-plaintext highlighter-rouge">nengo_dl.Converter()</code> API, Nengo-DL assumes a value of \(1\) for it.</p>

<p>Well‚Ä¶ this marks the end of this article, hope it was useful to you!</p>

<hr />
:ET